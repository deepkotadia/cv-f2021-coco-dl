{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv5_Custom_Training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrsaDfdVHzxt"
      },
      "source": [
        "# Custom Training with YOLOv5\n",
        "\n",
        "In this tutorial, we assemble a dataset and train a custom YOLOv5 model to recognize the objects in our dataset. To do so we will take the following steps:\n",
        "\n",
        "* Gather a dataset of images and label our dataset\n",
        "* Export our dataset to YOLOv5\n",
        "* Train YOLOv5 to recognize the objects in our dataset\n",
        "* Evaluate our YOLOv5 model's performance\n",
        "* Run test inference to view our model at work\n",
        "\n",
        "\n",
        "\n",
        "![](https://uploads-ssl.webflow.com/5f6bc60e665f54545a1e52a5/615627e5824c9c6195abfda9_computer-vision-cycle.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNveqeA1KXGy"
      },
      "source": [
        "# Step 1: Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTvDNSILZoN9",
        "outputId": "f5459b7e-9010-47ca-8665-9fa6b93783bc"
      },
      "source": [
        "#clone YOLOv5 and \n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%cd ..\n",
        "%pip install wandb\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 10200, done.\u001b[K\n",
            "remote: Total 10200 (delta 0), reused 0 (delta 0), pack-reused 10200\u001b[K\n",
            "Receiving objects: 100% (10200/10200), 10.47 MiB | 8.19 MiB/s, done.\n",
            "Resolving deltas: 100% (7068/7068), done.\n",
            "/content/yolov5\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 4.1 MB/s \n",
            "\u001b[?25h/content\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.7 MB 4.2 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.1-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 140 kB 50.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 180 kB 53.9 MB/s \n",
            "\u001b[?25hCollecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97 kB 6.6 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63 kB 460 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=d7f22106cba7987ea905f7975e5feb58958e2f632f1517d19ac807ffe2d4c32b\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=63a88924a9cacea3a9dde1e4f98bde41f7aab3a5991beec5c5b33c51cb488fb4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.24 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.1 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.7 yaspin-2.1.0\n",
            "Setup complete. Using torch 1.10.0+cu111 (Tesla P100-PCIE-16GB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5\n",
        "!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt\n",
        "%cd .."
      ],
      "metadata": {
        "id": "g6hRWdAkcfEt",
        "outputId": "6ab6f75e-d066-4a81-cfd1-d2fa262080e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Create a W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Create an account here: https://wandb.ai/authorize?signup=true\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=coco128.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=3, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v6.0-140-g19c56e6 torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeepkotadia\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mavid-voice-1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5/runs/zgj7utkw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/yolov5/wandb/run-20211213_093946-zgj7utkw\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 270 layers, 7235389 parameters, 7235389 gradients, 16.5 GFLOPs\n",
            "\n",
            "Transferred 349/349 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/coco128/labels/train2017.cache' images and labels... 128 found, 0 missing, 2 empty, 0 corrupted: 100% 128/128 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco128/labels/train2017.cache' images and labels... 128 found, 0 missing, 2 empty, 0 corrupted: 100% 128/128 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/train/exp2/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.27 anchors/target, 0.994 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/2     3.48G   0.04496   0.06916   0.02016       255       640: 100% 8/8 [00:05<00:00,  1.52it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 4/4 [00:02<00:00,  1.45it/s]\n",
            "                 all        128        929      0.708      0.518      0.622       0.41\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/2      4.3G   0.04522   0.06327   0.02255       156       640: 100% 8/8 [00:03<00:00,  2.43it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 4/4 [00:02<00:00,  1.48it/s]\n",
            "                 all        128        929      0.699      0.537      0.626      0.413\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/2      4.3G   0.04664   0.07331   0.02038       282       640: 100% 8/8 [00:03<00:00,  2.57it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 4/4 [00:02<00:00,  1.50it/s]\n",
            "                 all        128        929      0.698      0.541      0.628      0.417\n",
            "\n",
            "3 epochs completed in 0.007 hours.\n",
            "Optimizer stripped from runs/train/exp2/weights/last.pt, 14.9MB\n",
            "\n",
            "Validating runs/train/exp2/weights/best.pt...\n",
            "Optimizer stripped from runs/train/exp2/weights/best.pt, 14.9MB\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7225885 parameters, 0 gradients, 16.5 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 4/4 [00:05<00:00,  1.30s/it]\n",
            "                 all        128        929      0.698      0.542      0.629      0.417\n",
            "              person        128        254      0.822      0.669      0.773       0.51\n",
            "             bicycle        128          6      0.769      0.562      0.607      0.363\n",
            "                 car        128         46        0.8       0.37      0.468      0.211\n",
            "          motorcycle        128          5      0.685        0.6      0.795      0.578\n",
            "            airplane        128          6          1      0.808      0.995       0.71\n",
            "                 bus        128          7      0.637      0.714      0.721      0.608\n",
            "               train        128          3      0.815          1      0.995      0.682\n",
            "               truck        128         12      0.574       0.25       0.42      0.249\n",
            "                boat        128          6      0.754      0.333       0.44      0.135\n",
            "       traffic light        128         14      0.558      0.143      0.236      0.159\n",
            "           stop sign        128          2      0.626        0.5      0.828      0.713\n",
            "               bench        128          9      0.953      0.444      0.584      0.245\n",
            "                bird        128         16      0.886       0.97      0.985      0.649\n",
            "                 cat        128          4      0.906       0.75      0.828      0.706\n",
            "                 dog        128          9      0.929      0.667      0.866      0.558\n",
            "               horse        128          2      0.658          1      0.995      0.672\n",
            "            elephant        128         17      0.957      0.882       0.94      0.676\n",
            "                bear        128          1      0.574          1      0.995      0.995\n",
            "               zebra        128          4      0.856          1      0.995      0.952\n",
            "             giraffe        128          9      0.814      0.778      0.895      0.542\n",
            "            backpack        128          6          1      0.312      0.452      0.195\n",
            "            umbrella        128         18      0.763      0.611      0.725      0.397\n",
            "             handbag        128         19      0.612      0.105      0.176       0.11\n",
            "                 tie        128          7      0.924      0.571      0.693      0.434\n",
            "            suitcase        128          4          1      0.815      0.995      0.515\n",
            "             frisbee        128          5      0.613        0.8      0.798      0.723\n",
            "                skis        128          1      0.613          1      0.995      0.497\n",
            "           snowboard        128          7      0.987      0.714      0.769      0.574\n",
            "         sports ball        128          6      0.623        0.5      0.579      0.321\n",
            "                kite        128         10       0.59        0.5      0.557      0.214\n",
            "        baseball bat        128          4      0.486        0.5      0.302      0.141\n",
            "      baseball glove        128          7      0.432      0.429      0.374      0.218\n",
            "          skateboard        128          5      0.708        0.5      0.739      0.566\n",
            "       tennis racket        128          7      0.547      0.571      0.539      0.288\n",
            "              bottle        128         18      0.662      0.436      0.487      0.283\n",
            "          wine glass        128         16      0.641       0.75      0.735      0.375\n",
            "                 cup        128         36      0.845      0.361      0.502        0.3\n",
            "                fork        128          6       0.38      0.167      0.248      0.187\n",
            "               knife        128         16      0.809      0.625       0.68       0.46\n",
            "               spoon        128         22      0.817      0.409      0.547      0.256\n",
            "                bowl        128         28      0.753      0.571      0.623      0.457\n",
            "              banana        128          1          0          0      0.142     0.0142\n",
            "            sandwich        128          2          0          0      0.124     0.0946\n",
            "              orange        128          4          1          0      0.559      0.216\n",
            "            broccoli        128         11      0.322      0.182      0.334      0.288\n",
            "              carrot        128         24      0.695      0.475      0.615       0.37\n",
            "             hot dog        128          2      0.298        0.5      0.398      0.375\n",
            "               pizza        128          5      0.614          1      0.824      0.574\n",
            "               donut        128         14      0.691          1      0.949      0.858\n",
            "                cake        128          4      0.724          1      0.945      0.752\n",
            "               chair        128         35      0.502      0.514      0.488      0.223\n",
            "               couch        128          6      0.685       0.37      0.788      0.424\n",
            "        potted plant        128         14      0.844      0.773      0.793      0.458\n",
            "                 bed        128          3          1          0      0.641      0.279\n",
            "        dining table        128         13      0.834      0.462      0.473      0.293\n",
            "              toilet        128          2      0.524        0.5      0.557       0.49\n",
            "                  tv        128          2      0.751          1      0.995      0.846\n",
            "              laptop        128          3          1          0      0.426      0.185\n",
            "               mouse        128          2          1          0     0.0284     0.0227\n",
            "              remote        128          8      0.731      0.625      0.636      0.506\n",
            "          cell phone        128          8      0.552      0.164      0.377      0.199\n",
            "           microwave        128          3       0.44          1      0.995      0.798\n",
            "                oven        128          5      0.366        0.4      0.434      0.246\n",
            "                sink        128          6      0.347      0.167      0.264       0.15\n",
            "        refrigerator        128          5      0.741        0.8      0.814      0.456\n",
            "                book        128         29      0.614      0.138      0.295      0.134\n",
            "               clock        128          9      0.841      0.778      0.886      0.577\n",
            "                vase        128          2      0.232          1      0.663      0.597\n",
            "            scissors        128          1          1          0     0.0207    0.00415\n",
            "          teddy bear        128         21       0.87      0.381      0.629      0.357\n",
            "          toothbrush        128          5      0.982        0.6      0.661      0.405\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 354... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 ‚ñÅ‚ñÖ‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 ‚ñÅ‚ñÑ‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision ‚ñà‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall ‚ñÅ‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss ‚ñÅ‚ñÇ‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss ‚ñÅ‚ñà‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss ‚ñÖ‚ñÅ‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss ‚ñà‚ñÖ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss ‚ñà‚ñÑ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss ‚ñà‚ñÑ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 ‚ñÅ‚ñà‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 ‚ñÅ‚ñà‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 ‚ñà‚ñÖ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 0.62801\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.41656\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.69801\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.54127\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.04664\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.02038\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.07331\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.04103\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.01386\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.03968\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 7e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 7e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.09777\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 113 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mavid-voice-1\u001b[0m: \u001b[34mhttps://wandb.ai/deepkotadia/YOLOv5/runs/zgj7utkw\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211213_093946-zgj7utkw/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "Results saved to \u001b[1mruns/train/exp2\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Set up Custom Data with Labels"
      ],
      "metadata": {
        "id": "WzKxyCnSqEK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/Computer Vision 2021\")\n",
        "\n",
        "!ls\n",
        "!rm -rf /content/datasets\n",
        "!mkdir /content/datasets\n",
        "!cp coco_custom2_yolo.zip /content/datasets\n",
        "\n",
        "os.chdir(\"/content/datasets\")\n",
        "!ls\n",
        "\n",
        "!unzip coco_custom2_yolo.zip > /dev/null\n",
        "\n",
        "!rm coco_custom2_yolo.zip\n",
        "\n",
        "os.chdir(\"..\")\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks4U7Tv9mTa0",
        "outputId": "d6079609-40f9-469e-8618-f6b32fd4d458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            " coco_custom2_yolo.zip\n",
            " COCO-val2014-BBox-Results.gsheet\n",
            " ensemble_20.pth\n",
            " ensemble_30.pth\n",
            " ensemble_focal_20.pth\n",
            " ensemble_focal_40.pth\n",
            " fcn_test_10.pth\n",
            " fcn_test_scratch_10.pth\n",
            " fcn_test_scratch_15.pth\n",
            " fcn_test_scratch_data_augment_20.pth\n",
            " fcn_test_scratch_data_augment_dice_loss_10.pth\n",
            " fcn_test_scratch_data_augment_focal_loss_10.pth\n",
            " fcn_test_scratch_data_augment_focal_loss_v2_10.pth\n",
            " fcn_test_scratch_data_augment_focal_loss_v2_20.pth\n",
            " fcn_test_scratch_data_augment_v2_10.pth\n",
            " fcn_test_scratch_data_augment_v2_20.pth\n",
            " fcn_test_scratch_v2_10.pth\n",
            "'Final Project.gdoc'\n",
            "'Final Report Template.gdoc'\n",
            "'Progress Report.gdoc'\n",
            "'Project Proposal (Computer Vision 2021):.gdoc'\n",
            " segnet_20.pth\n",
            " segnet_30.pth\n",
            " segnet_40.pth\n",
            " segnet_60.pth\n",
            " segnet_focal_loss_10.pth\n",
            " segnet_focal_loss_40.pth\n",
            " segnet_focal_loss_60.pth\n",
            " segnet_test.pth\n",
            " train2017_custom2.zip\n",
            " train2017_custom.zip\n",
            " unet_20.pth\n",
            " unet_40.pth\n",
            " unet_60.pth\n",
            " unet_focal_20.pth\n",
            " unet_focal_40.pth\n",
            " unet_focal_60.pth\n",
            "'Useful Links.gdoc'\n",
            " val2017_custom2.zip\n",
            " val2017_custom.zip\n",
            " yolov5_coco_val2017_pretrained_results.zip\n",
            " yolov5_custom_coco_results.zip\n",
            "coco_custom2_yolo.zip\n",
            "datasets  drive  sample_data  yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv datasets/coco_custom2_yolo.yaml yolov5/data/"
      ],
      "metadata": {
        "id": "tmkQb5cCoIgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7yAi9hd-T4B"
      },
      "source": [
        "# Step 3: Train Our Custom YOLOv5 model\n",
        "\n",
        "Here, we are able to pass a number of arguments:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** Our dataset locaiton is saved in the `dataset.location`\n",
        "- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n",
        "- **cache:** cache images for faster training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaFNnxLJbq4J",
        "outputId": "301feb01-aeca-496d-e5eb-9bf4b4633560"
      },
      "source": [
        "%cd yolov5\n",
        "!python train.py --img 640 --batch 16 --epochs 15 --data coco_custom2_yolo.yaml --weights yolov5s.pt --cache\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeepkotadia\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=coco_custom2_yolo.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=15, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v6.0-140-g19c56e6 torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfallen-aardvark-2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5/runs/2f99hbwx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/yolov5/wandb/run-20211213_094301-2f99hbwx\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=20\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     67425  models.yolo.Detect                      [20, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 270 layers, 7073569 parameters, 7073569 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/train2017_custom2_yolo' images and labels...7771 found, 0 missing, 0 empty, 0 corrupted: 100% 7771/7771 [00:02<00:00, 3468.25it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../datasets/coco_custom2_yolo/labels/train2017_custom2_yolo.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (6.8GB ram): 100% 7771/7771 [00:25<00:00, 300.32it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/val2017_custom2_yolo' images and labels...2961 found, 0 missing, 0 empty, 0 corrupted: 100% 2961/2961 [00:01<00:00, 2413.26it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../datasets/coco_custom2_yolo/labels/val2017_custom2_yolo.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (2.6GB ram): 100% 2961/2961 [00:09<00:00, 299.16it/s]\n",
            "Plotting labels to runs/train/exp3/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.47 anchors/target, 0.996 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp3\u001b[0m\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      0/14      3.5G   0.07694    0.0573   0.05451       168       640: 100% 486/486 [03:02<00:00,  2.67it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:32<00:00,  2.82it/s]\n",
            "                 all       2961      16857      0.442      0.492       0.41      0.181\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      1/14     6.17G   0.06161   0.05433    0.0261        81       640: 100% 486/486 [02:55<00:00,  2.78it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:29<00:00,  3.19it/s]\n",
            "                 all       2961      16857      0.526      0.469      0.459       0.21\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      2/14     6.17G   0.05914   0.05755   0.02411       110       640: 100% 486/486 [02:53<00:00,  2.80it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:29<00:00,  3.14it/s]\n",
            "                 all       2961      16857      0.451      0.367      0.353       0.16\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      3/14     6.17G   0.05703   0.06098   0.02527        73       640: 100% 486/486 [02:52<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:31<00:00,  2.97it/s]\n",
            "                 all       2961      16857      0.491      0.404      0.398      0.191\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      4/14     6.17G   0.05512   0.06109   0.02331       133       640: 100% 486/486 [02:52<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:27<00:00,  3.44it/s]\n",
            "                 all       2961      16857      0.545      0.406      0.422      0.213\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      5/14     6.17G   0.05313   0.06051   0.02187        94       640: 100% 486/486 [02:52<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:27<00:00,  3.33it/s]\n",
            "                 all       2961      16857      0.557       0.44      0.453       0.23\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      6/14     6.17G   0.05178   0.05868   0.02023       130       640: 100% 486/486 [02:53<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:26<00:00,  3.47it/s]\n",
            "                 all       2961      16857      0.548      0.448      0.466      0.251\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      7/14     6.17G   0.05016   0.05817   0.01815       139       640: 100% 486/486 [02:52<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:26<00:00,  3.48it/s]\n",
            "                 all       2961      16857      0.585      0.464      0.492      0.267\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      8/14     6.17G   0.04898   0.05797   0.01699        89       640: 100% 486/486 [02:52<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:25<00:00,  3.62it/s]\n",
            "                 all       2961      16857      0.577      0.501      0.513      0.283\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      9/14     6.17G   0.04758   0.05663   0.01573        88       640: 100% 486/486 [02:52<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:24<00:00,  3.72it/s]\n",
            "                 all       2961      16857       0.64      0.484      0.525      0.295\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     10/14     6.17G    0.0467   0.05614   0.01459       112       640: 100% 486/486 [02:52<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:24<00:00,  3.77it/s]\n",
            "                 all       2961      16857       0.64      0.513      0.546      0.317\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     11/14     6.17G   0.04537    0.0543   0.01394       116       640: 100% 486/486 [02:52<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:24<00:00,  3.87it/s]\n",
            "                 all       2961      16857      0.647      0.515      0.552      0.323\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     12/14     6.17G    0.0444   0.05365   0.01267       170       640: 100% 486/486 [02:52<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:24<00:00,  3.84it/s]\n",
            "                 all       2961      16857      0.621      0.536      0.557       0.33\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     13/14     6.17G   0.04398   0.05337   0.01223       132       640: 100% 486/486 [02:53<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:23<00:00,  3.88it/s]\n",
            "                 all       2961      16857      0.647      0.521       0.56      0.337\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     14/14     6.17G   0.04318   0.05256    0.0116       134       640: 100% 486/486 [02:52<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:23<00:00,  3.88it/s]\n",
            "                 all       2961      16857      0.653      0.532      0.571      0.346\n",
            "\n",
            "15 epochs completed in 0.843 hours.\n",
            "Optimizer stripped from runs/train/exp3/weights/last.pt, 14.5MB\n",
            "Optimizer stripped from runs/train/exp3/weights/best.pt, 14.5MB\n",
            "\n",
            "Validating runs/train/exp3/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7064065 parameters, 0 gradients, 16.0 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:32<00:00,  2.86it/s]\n",
            "                 all       2961      16857      0.654      0.531      0.571      0.346\n",
            "              person       2961       7141      0.739      0.638      0.708       0.41\n",
            "             bicycle       2961        314      0.535      0.446      0.452      0.209\n",
            "                 car       2961       1843       0.65      0.545      0.576      0.321\n",
            "          motorcycle       2961        367      0.746       0.51      0.606      0.312\n",
            "            airplane       2961        143      0.839      0.692      0.804      0.529\n",
            "                 bus       2961        283      0.731      0.686      0.721      0.542\n",
            "               train       2961        190      0.808      0.641      0.712      0.473\n",
            "                boat       2961        424      0.579      0.325      0.362      0.153\n",
            "                bird       2961        427      0.573      0.436      0.443      0.247\n",
            "                 cat       2961        202      0.711      0.696      0.733      0.459\n",
            "                 dog       2961        218      0.695      0.533      0.623      0.426\n",
            "               horse       2961        272      0.777      0.599      0.663      0.455\n",
            "               sheep       2961        354      0.561      0.675      0.643      0.397\n",
            "                 cow       2961        372      0.635      0.653      0.652      0.415\n",
            "              bottle       2961       1013      0.547      0.413       0.42      0.237\n",
            "               chair       2961       1709      0.552      0.313      0.344      0.182\n",
            "               couch       2961        261      0.667      0.492      0.563      0.344\n",
            "        potted plant       2961        342      0.503      0.351      0.356      0.165\n",
            "        dining table       2961        694      0.649       0.35      0.395      0.233\n",
            "                  tv       2961        288      0.593      0.632      0.634      0.416\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 807... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision ‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall ‚ñÜ‚ñÖ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss ‚ñÖ‚ñÇ‚ñÖ‚ñà‚ñà‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss ‚ñà‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss ‚ñà‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss ‚ñÅ‚ñÉ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 ‚ñÉ‚ñÖ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 ‚ñÉ‚ñÖ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 0.57051\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.34642\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.65276\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.53224\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.04318\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.0116\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.05256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.04537\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.01305\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.04285\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 0.00139\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 0.00139\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.00139\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 497 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfallen-aardvark-2\u001b[0m: \u001b[34mhttps://wandb.ai/deepkotadia/YOLOv5/runs/2f99hbwx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211213_094301-2f99hbwx/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "Results saved to \u001b[1mruns/train/exp3\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5\n",
        "!python train.py --img 640 --batch 16 --epochs 15 --data coco_custom2_yolo.yaml --weights yolov5l.pt --cache\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQtPuGuHIO6T",
        "outputId": "53f9af87-c3a9-4d05-ed65-8d7fcedab71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeepkotadia\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l.pt, cfg=, data=coco_custom2_yolo.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=15, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v6.0-140-g19c56e6 torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mblooming-water-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5/runs/3cu58c00\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/yolov5/wandb/run-20211213_103531-3cu58c00\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.0/yolov5l.pt to yolov5l.pt...\n",
            "100% 89.2M/89.2M [00:01<00:00, 48.5MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=20\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
            "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
            "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
            "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
            "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
            " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
            " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
            " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
            " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]        \n",
            " 24      [17, 20, 23]  1    134625  models.yolo.Detect                      [20, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
            "Model Summary: 468 layers, 46240609 parameters, 46240609 gradients, 108.3 GFLOPs\n",
            "\n",
            "Transferred 607/613 items from yolov5l.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 101 weight, 104 weight (no decay), 104 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/train2017_custom2_yolo.cache' images and labels... 7771 found, 0 missing, 0 empty, 0 corrupted: 100% 7771/7771 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (6.8GB ram): 100% 7771/7771 [00:26<00:00, 298.13it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/val2017_custom2_yolo.cache' images and labels... 2961 found, 0 missing, 0 empty, 0 corrupted: 100% 2961/2961 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (2.6GB ram): 100% 2961/2961 [00:10<00:00, 283.32it/s]\n",
            "Plotting labels to runs/train/exp4/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.47 anchors/target, 0.996 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp4\u001b[0m\n",
            "Starting training for 15 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      0/14     9.89G   0.06958   0.05411   0.04629       105       640: 100% 486/486 [11:09<00:00,  1.38s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:06<00:00,  1.41it/s]\n",
            "                 all       2961      16857       0.56      0.588      0.568      0.298\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      1/14     12.2G   0.05532    0.0467   0.01668       153       640: 100% 486/486 [10:58<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:04<00:00,  1.44it/s]\n",
            "                 all       2961      16857      0.641      0.554       0.58      0.318\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      2/14     12.2G   0.05256   0.05153   0.01794       173       640: 100% 486/486 [10:55<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:05<00:00,  1.43it/s]\n",
            "                 all       2961      16857      0.576      0.428      0.455      0.237\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      3/14     12.2G   0.05132   0.05731   0.02079       124       640: 100% 486/486 [10:54<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:04<00:00,  1.44it/s]\n",
            "                 all       2961      16857      0.597      0.461      0.494       0.27\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      4/14     12.2G   0.04915   0.05626    0.0185        88       640: 100% 486/486 [10:54<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:04<00:00,  1.43it/s]\n",
            "                 all       2961      16857      0.595      0.501      0.514      0.297\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      5/14     12.2G   0.04681   0.05441   0.01679       127       640: 100% 486/486 [10:54<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:03<00:00,  1.47it/s]\n",
            "                 all       2961      16857      0.634      0.512      0.548      0.319\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      6/14     12.2G   0.04507    0.0529    0.0144       129       640: 100% 486/486 [10:54<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:02<00:00,  1.49it/s]\n",
            "                 all       2961      16857      0.657      0.542       0.58      0.354\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      7/14     12.2G   0.04342    0.0519    0.0131       136       640: 100% 486/486 [10:54<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:02<00:00,  1.50it/s]\n",
            "                 all       2961      16857      0.676      0.546      0.599      0.372\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      8/14     12.2G   0.04183   0.05036   0.01182       103       640: 100% 486/486 [10:54<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:01<00:00,  1.52it/s]\n",
            "                 all       2961      16857      0.689      0.568      0.613      0.387\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      9/14     12.2G   0.04001   0.04853   0.01061       107       640: 100% 486/486 [10:54<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:01<00:00,  1.52it/s]\n",
            "                 all       2961      16857      0.694      0.592      0.637      0.408\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     10/14     12.2G   0.03848   0.04704  0.009453       183       640: 100% 486/486 [10:54<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:00<00:00,  1.54it/s]\n",
            "                 all       2961      16857      0.712      0.582      0.638      0.418\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     11/14     12.2G    0.0371    0.0456  0.008353        86       640: 100% 486/486 [10:54<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:00<00:00,  1.54it/s]\n",
            "                 all       2961      16857      0.729      0.593      0.652      0.428\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     12/14     12.2G   0.03588   0.04374  0.007814        91       640: 100% 486/486 [10:54<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:59<00:00,  1.55it/s]\n",
            "                 all       2961      16857      0.738      0.602      0.659       0.44\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     13/14     12.2G   0.03488    0.0425  0.007197       144       640: 100% 486/486 [10:54<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:00<00:00,  1.54it/s]\n",
            "                 all       2961      16857      0.743      0.609      0.665      0.447\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     14/14     12.2G   0.03404   0.04191   0.00678       113       640: 100% 486/486 [10:54<00:00,  1.35s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:00<00:00,  1.54it/s]\n",
            "                 all       2961      16857       0.74      0.621      0.668      0.452\n",
            "\n",
            "15 epochs completed in 3.006 hours.\n",
            "Optimizer stripped from runs/train/exp4/weights/last.pt, 93.0MB\n",
            "Optimizer stripped from runs/train/exp4/weights/best.pt, 93.0MB\n",
            "\n",
            "Validating runs/train/exp4/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model Summary: 367 layers, 46210593 parameters, 0 gradients, 108.1 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:10<00:00,  1.31it/s]\n",
            "                 all       2961      16857      0.739      0.621      0.668      0.452\n",
            "              person       2961       7141      0.757      0.715      0.767      0.496\n",
            "             bicycle       2961        314       0.72      0.516      0.561      0.296\n",
            "                 car       2961       1843      0.723      0.609      0.669      0.417\n",
            "          motorcycle       2961        367      0.782      0.654      0.718      0.412\n",
            "            airplane       2961        143       0.85      0.769      0.859      0.629\n",
            "                 bus       2961        283        0.8      0.739      0.794      0.634\n",
            "               train       2961        190      0.885      0.747      0.831      0.609\n",
            "                boat       2961        424      0.626      0.445      0.468      0.238\n",
            "                bird       2961        427      0.702       0.48      0.539      0.337\n",
            "                 cat       2961        202      0.873      0.822      0.865      0.644\n",
            "                 dog       2961        218      0.817      0.706      0.758      0.584\n",
            "               horse       2961        272       0.88      0.701      0.807      0.587\n",
            "               sheep       2961        354      0.672      0.749      0.745      0.517\n",
            "                 cow       2961        372      0.736      0.728      0.767       0.55\n",
            "              bottle       2961       1013      0.645      0.491      0.515      0.329\n",
            "               chair       2961       1709      0.624      0.389      0.437      0.259\n",
            "               couch       2961        261      0.734       0.57      0.628      0.423\n",
            "        potted plant       2961        342      0.553      0.447       0.44      0.237\n",
            "        dining table       2961        694      0.627      0.448      0.447      0.292\n",
            "                  tv       2961        288      0.784      0.692      0.746      0.547\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1761... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 ‚ñÖ‚ñÖ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 ‚ñÉ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall ‚ñá‚ñÜ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss ‚ñá‚ñÉ‚ñÖ‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss ‚ñá‚ñÑ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss ‚ñÅ‚ñÉ‚ñà‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 ‚ñÉ‚ñÖ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 ‚ñÉ‚ñÖ‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 ‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 0.66822\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.4519\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.73977\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.62051\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.03404\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.00678\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.04191\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.03951\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.00993\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.04107\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 0.00139\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 0.00139\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.00139\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 497 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mblooming-water-3\u001b[0m: \u001b[34mhttps://wandb.ai/deepkotadia/YOLOv5/runs/3cu58c00\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211213_103531-3cu58c00/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "Results saved to \u001b[1mruns/train/exp4\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtmS7_TXFsT3"
      },
      "source": [
        "#Run on Val Set  With Trained Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWjjiBcic3Vz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c219b2b0-0685-4845-88f6-de2e59eecdfc"
      },
      "source": [
        "%cd yolov5\n",
        "!python val.py --weights runs/train/exp3/weights/best.pt --data coco_custom2_yolo.yaml --img 640 --iou 0.65 --half\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/coco_custom2_yolo.yaml, weights=['runs/train/exp3/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 üöÄ v6.0-140-g19c56e6 torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7064065 parameters, 0 gradients, 16.0 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/val2017_custom2_yolo.cache' images and labels... 2961 found, 0 missing, 0 empty, 0 corrupted: 100% 2961/2961 [00:00<?, ?it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:32<00:00,  2.87it/s]\n",
            "                 all       2961      16857       0.67      0.516      0.565      0.344\n",
            "              person       2961       7141      0.748       0.63      0.703      0.406\n",
            "             bicycle       2961        314      0.552      0.424      0.443      0.208\n",
            "                 car       2961       1843      0.654      0.533      0.569      0.317\n",
            "          motorcycle       2961        367       0.76      0.493      0.602      0.309\n",
            "            airplane       2961        143      0.857      0.673        0.8      0.527\n",
            "                 bus       2961        283      0.751      0.678      0.719      0.541\n",
            "               train       2961        190      0.823      0.637      0.711      0.471\n",
            "                boat       2961        424      0.593       0.29      0.354      0.151\n",
            "                bird       2961        427      0.579      0.424      0.437      0.245\n",
            "                 cat       2961        202      0.741      0.683      0.728      0.457\n",
            "                 dog       2961        218      0.717      0.505      0.617      0.423\n",
            "               horse       2961        272      0.795      0.596      0.659      0.452\n",
            "               sheep       2961        354      0.577      0.661      0.637      0.395\n",
            "                 cow       2961        372      0.636      0.634      0.644      0.411\n",
            "              bottle       2961       1013       0.56      0.393      0.416      0.235\n",
            "               chair       2961       1709      0.566      0.297      0.337      0.179\n",
            "               couch       2961        261      0.689      0.467      0.559      0.342\n",
            "        potted plant       2961        342      0.527      0.339      0.351      0.163\n",
            "        dining table       2961        694      0.674       0.34      0.392      0.231\n",
            "                  tv       2961        288      0.603      0.628       0.63      0.414\n",
            "Speed: 0.1ms pre-process, 3.4ms inference, 2.0ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5\n",
        "!python val.py --weights runs/train/exp4/weights/best.pt --data coco_custom2_yolo.yaml --img 640 --iou 0.65 --half\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8VUR3ikzO3W",
        "outputId": "817fbafc-057f-4eb0-e369-31210d97e2e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/coco_custom2_yolo.yaml, weights=['runs/train/exp4/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 üöÄ v6.0-140-g19c56e6 torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 367 layers, 46210593 parameters, 0 gradients, 108.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/val2017_custom2_yolo.cache' images and labels... 2961 found, 0 missing, 0 empty, 0 corrupted: 100% 2961/2961 [00:00<?, ?it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:56<00:00,  1.64it/s]\n",
            "                 all       2961      16857      0.747      0.611      0.665       0.45\n",
            "              person       2961       7141      0.763      0.705      0.762      0.493\n",
            "             bicycle       2961        314       0.72       0.51       0.56      0.295\n",
            "                 car       2961       1843      0.733      0.596      0.664      0.414\n",
            "          motorcycle       2961        367      0.778      0.638      0.711       0.41\n",
            "            airplane       2961        143      0.828      0.776      0.857      0.628\n",
            "                 bus       2961        283      0.807      0.731      0.792      0.632\n",
            "               train       2961        190      0.903      0.736      0.832      0.609\n",
            "                boat       2961        424      0.638      0.421      0.465      0.235\n",
            "                bird       2961        427      0.728      0.475      0.536      0.336\n",
            "                 cat       2961        202       0.88      0.812      0.867      0.644\n",
            "                 dog       2961        218      0.822      0.693      0.756      0.582\n",
            "               horse       2961        272      0.875      0.695      0.801      0.583\n",
            "               sheep       2961        354      0.672      0.743      0.743      0.515\n",
            "                 cow       2961        372      0.752      0.718      0.764      0.547\n",
            "              bottle       2961       1013      0.648      0.474      0.511      0.326\n",
            "               chair       2961       1709      0.636       0.38      0.432      0.256\n",
            "               couch       2961        261      0.756      0.552      0.625      0.422\n",
            "        potted plant       2961        342      0.568      0.444      0.436      0.235\n",
            "        dining table       2961        694      0.641      0.431      0.443      0.289\n",
            "                  tv       2961        288      0.796      0.688      0.744      0.545\n",
            "Speed: 0.1ms pre-process, 12.4ms inference, 1.6ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp2\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extension 1: Test-Time Augmentation"
      ],
      "metadata": {
        "id": "C_Jqgl9Yz25Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5\n",
        "!python val.py --weights runs/train/exp3/weights/best.pt --data coco_custom2_yolo.yaml --img 832 --augment --iou 0.65 --half\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz50Wmd0zg9a",
        "outputId": "d14e5e71-333f-4472-f771-9c307fabb74e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/coco_custom2_yolo.yaml, weights=['runs/train/exp3/weights/best.pt'], batch_size=32, imgsz=832, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=True, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 üöÄ v6.0-140-g19c56e6 torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7064065 parameters, 0 gradients, 16.0 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/val2017_custom2_yolo.cache' images and labels... 2961 found, 0 missing, 0 empty, 0 corrupted: 100% 2961/2961 [00:00<?, ?it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:05<00:00,  1.42it/s]\n",
            "                 all       2961      16857      0.655       0.56        0.6       0.37\n",
            "              person       2961       7141      0.709      0.661      0.725      0.427\n",
            "             bicycle       2961        314       0.61      0.427      0.482      0.226\n",
            "                 car       2961       1843      0.632       0.57      0.605      0.348\n",
            "          motorcycle       2961        367      0.774      0.561      0.659      0.347\n",
            "            airplane       2961        143      0.846      0.748      0.805      0.526\n",
            "                 bus       2961        283      0.698      0.721      0.757      0.576\n",
            "               train       2961        190      0.827      0.653      0.765      0.521\n",
            "                boat       2961        424      0.588      0.333       0.39      0.172\n",
            "                bird       2961        427      0.564      0.433       0.46      0.276\n",
            "                 cat       2961        202      0.739      0.767      0.765      0.472\n",
            "                 dog       2961        218      0.658      0.591      0.633      0.443\n",
            "               horse       2961        272      0.772      0.647       0.72      0.479\n",
            "               sheep       2961        354      0.568      0.698      0.666      0.416\n",
            "                 cow       2961        372      0.591      0.704      0.686      0.462\n",
            "              bottle       2961       1013      0.547      0.486       0.47      0.273\n",
            "               chair       2961       1709      0.552      0.334      0.368      0.201\n",
            "               couch       2961        261      0.656      0.483      0.562      0.354\n",
            "        potted plant       2961        342      0.528      0.372      0.387      0.183\n",
            "        dining table       2961        694      0.674      0.346      0.416      0.251\n",
            "                  tv       2961        288      0.569      0.663      0.669      0.448\n",
            "Speed: 0.2ms pre-process, 12.6ms inference, 3.8ms NMS per image at shape (32, 3, 832, 832)\n",
            "Results saved to \u001b[1mruns/val/exp3\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5\n",
        "!python val.py --weights runs/train/exp4/weights/best.pt --data coco_custom2_yolo.yaml --img 832 --augment --iou 0.65 --half\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgvV45uazhi_",
        "outputId": "afd97f0e-a9dd-41c8-d072-a4adae8076df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/coco_custom2_yolo.yaml, weights=['runs/train/exp4/weights/best.pt'], batch_size=32, imgsz=832, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=True, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 üöÄ v6.0-140-g19c56e6 torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 367 layers, 46210593 parameters, 0 gradients, 108.1 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/val2017_custom2_yolo.cache' images and labels... 2961 found, 0 missing, 0 empty, 0 corrupted: 100% 2961/2961 [00:00<?, ?it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [02:33<00:00,  1.65s/it]\n",
            "                 all       2961      16857       0.72       0.64      0.685      0.472\n",
            "              person       2961       7141      0.756       0.72      0.781      0.518\n",
            "             bicycle       2961        314      0.682      0.519      0.553      0.305\n",
            "                 car       2961       1843      0.728       0.62      0.681      0.436\n",
            "          motorcycle       2961        367      0.761       0.67       0.74      0.451\n",
            "            airplane       2961        143      0.817      0.846      0.881      0.656\n",
            "                 bus       2961        283      0.764      0.766      0.815      0.662\n",
            "               train       2961        190      0.907      0.753       0.86      0.648\n",
            "                boat       2961        424       0.61      0.432      0.496      0.246\n",
            "                bird       2961        427      0.718      0.464      0.546      0.351\n",
            "                 cat       2961        202      0.793      0.861      0.867       0.66\n",
            "                 dog       2961        218      0.768      0.752      0.775      0.598\n",
            "               horse       2961        272      0.846      0.754      0.829      0.613\n",
            "               sheep       2961        354      0.638      0.749      0.748      0.522\n",
            "                 cow       2961        372      0.723      0.745      0.776      0.561\n",
            "              bottle       2961       1013      0.612      0.513      0.533      0.352\n",
            "               chair       2961       1709      0.591      0.425      0.466      0.285\n",
            "               couch       2961        261      0.728      0.544      0.629      0.445\n",
            "        potted plant       2961        342      0.546      0.509      0.486      0.269\n",
            "        dining table       2961        694      0.651      0.448      0.477      0.312\n",
            "                  tv       2961        288      0.763      0.701       0.76      0.557\n",
            "Speed: 0.2ms pre-process, 44.2ms inference, 2.5ms NMS per image at shape (32, 3, 832, 832)\n",
            "Results saved to \u001b[1mruns/val/exp4\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extension 2: Model Ensembling"
      ],
      "metadata": {
        "id": "dB3kThlQ0gBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd yolov5\n",
        "!python val.py --weights runs/train/exp3/weights/best.pt runs/train/exp4/weights/best.pt --data coco_custom2_yolo.yaml --img 640 --iou 0.65 --half\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEsGMzYr0k9G",
        "outputId": "83699ddd-de4d-4edf-a95e-59d8b973dd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/coco_custom2_yolo.yaml, weights=['runs/train/exp3/weights/best.pt', 'runs/train/exp4/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 üöÄ v6.0-140-g19c56e6 torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7064065 parameters, 0 gradients, 16.0 GFLOPs\n",
            "Fusing layers... \n",
            "Model Summary: 367 layers, 46210593 parameters, 0 gradients, 108.1 GFLOPs\n",
            "Ensemble created with ['runs/train/exp3/weights/best.pt', 'runs/train/exp4/weights/best.pt']\n",
            "\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/val2017_custom2_yolo.cache' images and labels... 2961 found, 0 missing, 0 empty, 0 corrupted: 100% 2961/2961 [00:00<?, ?it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [01:09<00:00,  1.33it/s]\n",
            "                 all       2961      16857      0.685       0.62       0.65      0.436\n",
            "              person       2961       7141      0.729      0.707      0.751      0.483\n",
            "             bicycle       2961        314      0.613      0.519      0.543      0.285\n",
            "                 car       2961       1843      0.673      0.593      0.638      0.394\n",
            "          motorcycle       2961        367      0.762      0.646      0.697      0.403\n",
            "            airplane       2961        143      0.789      0.783      0.847       0.62\n",
            "                 bus       2961        283      0.764      0.742      0.789      0.624\n",
            "               train       2961        190      0.818      0.721      0.806       0.59\n",
            "                boat       2961        424      0.605      0.415      0.451      0.227\n",
            "                bird       2961        427      0.639      0.482      0.513      0.319\n",
            "                 cat       2961        202      0.805      0.827      0.842      0.618\n",
            "                 dog       2961        218      0.759      0.702      0.743      0.561\n",
            "               horse       2961        272      0.793       0.69      0.757      0.556\n",
            "               sheep       2961        354      0.605      0.758      0.721      0.499\n",
            "                 cow       2961        372       0.66       0.75      0.748      0.526\n",
            "              bottle       2961       1013      0.587      0.495      0.504      0.322\n",
            "               chair       2961       1709      0.586      0.391      0.427      0.252\n",
            "               couch       2961        261      0.689      0.544      0.617      0.407\n",
            "        potted plant       2961        342      0.551      0.465      0.429      0.227\n",
            "        dining table       2961        694      0.635      0.437      0.445      0.287\n",
            "                  tv       2961        288      0.635      0.726      0.728      0.523\n",
            "Speed: 0.1ms pre-process, 15.8ms inference, 2.5ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp5\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extension 3: Transfer Learning"
      ],
      "metadata": {
        "id": "fNjPHTJBL8Ia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train from scratch on Pascal VOC dataset\n",
        "%cd yolov5\n",
        "!python train.py --img 640 --batch 16 --epochs 7 --weights '' --cfg yolov5s.yaml --data VOC.yaml --cache\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08RlogDUL7ik",
        "outputId": "ed35fbb5-33cf-4cb1-bc71-c3edd496b8c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeepkotadia\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=, cfg=yolov5s.yaml, data=VOC.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=7, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v6.0-141-ge8ef8fb torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mzany-planet-11\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5/runs/3unmkojn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/yolov5/wandb/run-20211214_030246-3unmkojn\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=20\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     67425  models.yolo.Detect                      [20, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 270 layers, 7073569 parameters, 7073569 gradients, 16.0 GFLOPs\n",
            "\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/VOC/labels/train2007.cache' images and labels... 16551 found, 0 missing, 0 empty, 0 corrupted: 100% 16551/16551 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (14.7GB ram): 100% 16551/16551 [00:55<00:00, 299.84it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/VOC/labels/test2007.cache' images and labels... 4952 found, 0 missing, 0 empty, 0 corrupted: 100% 4952/4952 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (4.4GB ram): 100% 4952/4952 [00:14<00:00, 341.80it/s]\n",
            "Plotting labels to runs/train/exp8/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.99 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp8\u001b[0m\n",
            "Starting training for 7 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/6     3.49G   0.08985   0.04391   0.06805        31       640: 100% 1035/1035 [06:25<00:00,  2.69it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 155/155 [00:40<00:00,  3.86it/s]\n",
            "                 all       4952      12032     0.0014     0.0113   0.000716   0.000152\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/6     6.14G   0.08047   0.04606   0.06255        36       640: 100% 1035/1035 [06:13<00:00,  2.77it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 155/155 [00:42<00:00,  3.63it/s]\n",
            "                 all       4952      12032     0.0593     0.0764    0.00765    0.00184\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/6     6.14G   0.06695   0.04611   0.05852        29       640: 100% 1035/1035 [06:09<00:00,  2.80it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 155/155 [00:49<00:00,  3.14it/s]\n",
            "                 all       4952      12032      0.158      0.144     0.0371     0.0109\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/6     6.14G   0.06053   0.04442   0.05227        26       640: 100% 1035/1035 [06:08<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 155/155 [00:51<00:00,  3.04it/s]\n",
            "                 all       4952      12032      0.164      0.179     0.0872       0.03\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/6     6.14G   0.05735   0.04263   0.04672        46       640: 100% 1035/1035 [06:08<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 155/155 [00:43<00:00,  3.57it/s]\n",
            "                 all       4952      12032      0.256      0.239      0.134      0.048\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/6     6.14G   0.05462   0.04195   0.04255        50       640: 100% 1035/1035 [06:07<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 155/155 [00:43<00:00,  3.59it/s]\n",
            "                 all       4952      12032      0.283       0.31      0.202     0.0805\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/6     6.14G   0.05251   0.04125   0.03934        37       640: 100% 1035/1035 [06:07<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 155/155 [00:41<00:00,  3.74it/s]\n",
            "                 all       4952      12032      0.322      0.317      0.244      0.101\n",
            "\n",
            "7 epochs completed in 0.814 hours.\n",
            "Optimizer stripped from runs/train/exp8/weights/last.pt, 14.5MB\n",
            "\n",
            "Validating runs/train/exp8/weights/best.pt...\n",
            "Optimizer stripped from runs/train/exp8/weights/best.pt, 14.5MB\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7064065 parameters, 0 gradients, 16.0 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 155/155 [00:48<00:00,  3.17it/s]\n",
            "                 all       4952      12032      0.322      0.317      0.244      0.102\n",
            "           aeroplane       4952        285      0.487      0.439      0.375      0.134\n",
            "             bicycle       4952        337      0.351      0.359      0.271      0.109\n",
            "                bird       4952        459      0.226      0.146      0.116     0.0415\n",
            "                boat       4952        263      0.314     0.0532     0.0759     0.0274\n",
            "              bottle       4952        469      0.306     0.0341     0.0372     0.0122\n",
            "                 bus       4952        213      0.307      0.423      0.264      0.128\n",
            "                 car       4952       1201      0.464      0.669      0.619        0.3\n",
            "                 cat       4952        358      0.361      0.293      0.269      0.116\n",
            "               chair       4952        756      0.329      0.226       0.19     0.0688\n",
            "                 cow       4952        244      0.243       0.57      0.289      0.139\n",
            "         diningtable       4952        206      0.154     0.0243     0.0775     0.0211\n",
            "                 dog       4952        489       0.29      0.278      0.208      0.089\n",
            "               horse       4952        348      0.251      0.307      0.184     0.0558\n",
            "           motorbike       4952        325      0.368      0.468      0.303      0.133\n",
            "              person       4952       4528       0.45       0.62      0.554      0.216\n",
            "         pottedplant       4952        480      0.292     0.0187     0.0404     0.0125\n",
            "               sheep       4952        242      0.173      0.512      0.208      0.101\n",
            "                sofa       4952        239      0.306      0.117      0.142     0.0603\n",
            "               train       4952        282      0.469      0.348      0.343      0.127\n",
            "           tvmonitor       4952        308      0.291      0.429       0.31      0.141\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 4574... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss ‚ñà‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss ‚ñÖ‚ñà‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss ‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss ‚ñÑ‚ñà‚ñà‚ñá‚ñÉ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 0.24375\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.1015\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.32166\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.31664\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.05251\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.03934\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.04125\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.04972\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.0291\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.02508\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 0.00269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 0.00269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.00269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 241 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mzany-planet-11\u001b[0m: \u001b[34mhttps://wandb.ai/deepkotadia/YOLOv5/runs/3unmkojn\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211214_030246-3unmkojn/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "Results saved to \u001b[1mruns/train/exp8\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train using Pascal VOC weights on custom coco\n",
        "%cd yolov5\n",
        "!python train.py --img 640 --batch 16 --epochs 7 --data coco_custom2_yolo.yaml --weights runs/train/exp8/weights/best.pt --cache\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkPw1CVuNJeT",
        "outputId": "19a8280c-a6e4-4dcd-943d-0c84f54f623d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeepkotadia\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=runs/train/exp8/weights/best.pt, cfg=, data=coco_custom2_yolo.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=7, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v6.0-141-ge8ef8fb torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcolorful-leaf-12\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5/runs/1j306qdf\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/yolov5/wandb/run-20211214_035449-1j306qdf\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     67425  models.yolo.Detect                      [20, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 270 layers, 7073569 parameters, 7073569 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 349/349 items from runs/train/exp8/weights/best.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/train2017_custom2_yolo.cache' images and labels... 7771 found, 0 missing, 0 empty, 0 corrupted: 100% 7771/7771 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (6.8GB ram): 100% 7771/7771 [00:28<00:00, 277.28it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/val2017_custom2_yolo.cache' images and labels... 2961 found, 0 missing, 0 empty, 0 corrupted: 100% 2961/2961 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (2.6GB ram): 100% 2961/2961 [00:11<00:00, 261.79it/s]\n",
            "Plotting labels to runs/train/exp9/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.47 anchors/target, 0.996 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp9\u001b[0m\n",
            "Starting training for 7 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/6      3.5G   0.06866   0.07202    0.0699       168       640: 100% 486/486 [03:01<00:00,  2.68it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:34<00:00,  2.72it/s]\n",
            "                 all       2961      16857      0.495      0.082     0.0665     0.0275\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/6     6.17G   0.06749   0.07058   0.05067        81       640: 100% 486/486 [02:54<00:00,  2.78it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:32<00:00,  2.89it/s]\n",
            "                 all       2961      16857      0.222      0.178      0.103     0.0385\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/6     6.17G   0.06825   0.06969   0.04561       110       640: 100% 486/486 [02:52<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:35<00:00,  2.60it/s]\n",
            "                 all       2961      16857      0.241      0.174      0.117     0.0427\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/6     6.17G   0.06745   0.06917   0.04245        73       640: 100% 486/486 [02:52<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:30<00:00,  3.06it/s]\n",
            "                 all       2961      16857      0.179      0.247      0.142     0.0536\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/6     6.17G   0.06656   0.06948   0.04018       133       640: 100% 486/486 [02:52<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:29<00:00,  3.11it/s]\n",
            "                 all       2961      16857      0.249      0.228      0.163     0.0622\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/6     6.17G   0.06473   0.06896   0.03849        94       640: 100% 486/486 [02:52<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:31<00:00,  2.97it/s]\n",
            "                 all       2961      16857      0.323      0.227      0.193     0.0769\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/6     6.17G   0.06322   0.06768   0.03681       130       640: 100% 486/486 [02:52<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:28<00:00,  3.29it/s]\n",
            "                 all       2961      16857      0.301      0.258      0.213     0.0868\n",
            "\n",
            "7 epochs completed in 0.404 hours.\n",
            "Optimizer stripped from runs/train/exp9/weights/last.pt, 14.5MB\n",
            "Optimizer stripped from runs/train/exp9/weights/best.pt, 14.5MB\n",
            "\n",
            "Validating runs/train/exp9/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7064065 parameters, 0 gradients, 16.0 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:34<00:00,  2.72it/s]\n",
            "                 all       2961      16857      0.309      0.253      0.213     0.0868\n",
            "              person       2961       7141      0.362      0.482      0.419      0.162\n",
            "             bicycle       2961        314      0.189      0.115     0.0725     0.0234\n",
            "                 car       2961       1843      0.261      0.334      0.259     0.0987\n",
            "          motorcycle       2961        367      0.231      0.277      0.188     0.0572\n",
            "            airplane       2961        143      0.403       0.42      0.387      0.147\n",
            "                 bus       2961        283      0.393      0.396      0.349      0.183\n",
            "               train       2961        190      0.372      0.458       0.38      0.164\n",
            "                boat       2961        424      0.159      0.104     0.0527     0.0128\n",
            "                bird       2961        427      0.166      0.105     0.0548       0.02\n",
            "                 cat       2961        202      0.417      0.134      0.209     0.0721\n",
            "                 dog       2961        218      0.596     0.0734      0.202      0.088\n",
            "               horse       2961        272      0.374      0.287      0.274      0.112\n",
            "               sheep       2961        354      0.146      0.506      0.292      0.105\n",
            "                 cow       2961        372      0.261      0.366      0.256      0.125\n",
            "              bottle       2961       1013      0.122      0.142     0.0543     0.0157\n",
            "               chair       2961       1709      0.309     0.0404     0.0671     0.0229\n",
            "               couch       2961        261      0.352      0.107       0.13     0.0549\n",
            "        potted plant       2961        342      0.301     0.0906     0.0757     0.0229\n",
            "        dining table       2961        694      0.461      0.197      0.192     0.0838\n",
            "                  tv       2961        288        0.3      0.424      0.345      0.166\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 5620... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall ‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss ‚ñà‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss ‚ñá‚ñÖ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 0.21308\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.08676\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.30089\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.25756\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.06322\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.03681\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.06768\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.06374\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.03107\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.05045\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 0.00269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 0.00269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.00269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 241 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mcolorful-leaf-12\u001b[0m: \u001b[34mhttps://wandb.ai/deepkotadia/YOLOv5/runs/1j306qdf\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211214_035449-1j306qdf/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "Results saved to \u001b[1mruns/train/exp9\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate on custom coco with transfer learning weights\n",
        "%cd yolov5\n",
        "!python val.py --weights runs/train/exp9/weights/best.pt --data coco_custom2_yolo.yaml --img 640 --iou 0.65 --half\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxQ01zAcNyFu",
        "outputId": "15fdb02a-e74b-4870-d435-4c1edb25d1fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/coco_custom2_yolo.yaml, weights=['runs/train/exp9/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 üöÄ v6.0-141-ge8ef8fb torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7064065 parameters, 0 gradients, 16.0 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/val2017_custom2_yolo.cache' images and labels... 2961 found, 0 missing, 0 empty, 0 corrupted: 100% 2961/2961 [00:00<?, ?it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:34<00:00,  2.69it/s]\n",
            "                 all       2961      16857      0.288      0.255      0.205     0.0837\n",
            "              person       2961       7141      0.315      0.489      0.404      0.156\n",
            "             bicycle       2961        314      0.169      0.118     0.0691     0.0229\n",
            "                 car       2961       1843      0.223      0.341      0.243     0.0926\n",
            "          motorcycle       2961        367      0.201      0.275      0.175     0.0533\n",
            "            airplane       2961        143      0.338       0.42      0.365      0.139\n",
            "                 bus       2961        283      0.372      0.392      0.342      0.179\n",
            "               train       2961        190      0.358      0.458      0.369       0.16\n",
            "                boat       2961        424      0.144      0.108     0.0481     0.0111\n",
            "                bird       2961        427      0.156       0.11     0.0505     0.0186\n",
            "                 cat       2961        202      0.421      0.134      0.206     0.0702\n",
            "                 dog       2961        218      0.607     0.0734        0.2     0.0866\n",
            "               horse       2961        272      0.361      0.287      0.268      0.109\n",
            "               sheep       2961        354      0.129       0.52      0.281      0.101\n",
            "                 cow       2961        372      0.248      0.366       0.25      0.122\n",
            "              bottle       2961       1013      0.108      0.143     0.0494     0.0143\n",
            "               chair       2961       1709      0.286     0.0398     0.0618     0.0209\n",
            "               couch       2961        261      0.324      0.107      0.124     0.0521\n",
            "        potted plant       2961        342      0.292     0.0936     0.0725     0.0219\n",
            "        dining table       2961        694      0.424      0.199      0.186     0.0802\n",
            "                  tv       2961        288      0.281      0.424      0.334      0.161\n",
            "Speed: 0.1ms pre-process, 3.4ms inference, 3.1ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp4\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train from scratch on VisDrone dataset\n",
        "%cd yolov5\n",
        "!python train.py --img 640 --batch 16 --epochs 7 --weights '' --cfg yolov5s.yaml --data VisDrone.yaml --cache\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78zTPGiJNfEn",
        "outputId": "d6d807a7-266b-4b50-c2be-2fcc9b3f6d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeepkotadia\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=, cfg=yolov5s.yaml, data=VisDrone.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=7, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v6.0-141-ge8ef8fb torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mapricot-sea-8\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5/runs/1klnp9u2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/yolov5/wandb/run-20211214_013959-1klnp9u2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "\n",
            "WARNING: Dataset not found, nonexistent paths: ['/content/datasets/VisDrone/VisDrone2019-DET-val/images']\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/VisDrone2019-DET-train.zip to ../datasets/VisDrone/VisDrone2019-DET-train.zip...\n",
            "100% 1.44G/1.44G [00:58<00:00, 26.4MB/s]\n",
            "Unzipping ../datasets/VisDrone/VisDrone2019-DET-train.zip...\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/VisDrone2019-DET-val.zip to ../datasets/VisDrone/VisDrone2019-DET-val.zip...\n",
            "100% 77.9M/77.9M [00:06<00:00, 13.6MB/s]\n",
            "Unzipping ../datasets/VisDrone/VisDrone2019-DET-val.zip...\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/VisDrone2019-DET-test-dev.zip to ../datasets/VisDrone/VisDrone2019-DET-test-dev.zip...\n",
            "100% 297M/297M [00:16<00:00, 18.9MB/s]\n",
            "Unzipping ../datasets/VisDrone/VisDrone2019-DET-test-dev.zip...\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/VisDrone2019-DET-test-challenge.zip to ../datasets/VisDrone/VisDrone2019-DET-test-challenge.zip...\n",
            "100% 292M/292M [00:16<00:00, 19.0MB/s]\n",
            "Unzipping ../datasets/VisDrone/VisDrone2019-DET-test-challenge.zip...\n",
            "Converting ../datasets/VisDrone/VisDrone2019-DET-train: 6471it [00:53, 121.48it/s]\n",
            "Converting ../datasets/VisDrone/VisDrone2019-DET-val: 548it [00:05, 92.02it/s] \n",
            "Converting ../datasets/VisDrone/VisDrone2019-DET-test-dev: 1610it [00:11, 137.69it/s]\n",
            "Dataset autodownload success, saved to ../datasets\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=10\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     40455  models.yolo.Detect                      [10, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 270 layers, 7046599 parameters, 7046599 gradients, 15.9 GFLOPs\n",
            "\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/VisDrone/VisDrone2019-DET-train/labels' images and labels...6471 found, 0 missing, 0 empty, 0 corrupted: 100% 6471/6471 [00:02<00:00, 2779.65it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: ../datasets/VisDrone/VisDrone2019-DET-train/images/0000137_02220_d_0000163.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: ../datasets/VisDrone/VisDrone2019-DET-train/images/0000140_00118_d_0000002.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: ../datasets/VisDrone/VisDrone2019-DET-train/images/9999945_00000_d_0000114.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: ../datasets/VisDrone/VisDrone2019-DET-train/images/9999987_00000_d_0000049.jpg: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../datasets/VisDrone/VisDrone2019-DET-train/labels.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (5.2GB ram): 100% 6471/6471 [01:14<00:00, 87.07it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/VisDrone/VisDrone2019-DET-val/labels' images and labels...548 found, 0 missing, 0 empty, 0 corrupted: 100% 548/548 [00:00<00:00, 1089.23it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: ../datasets/VisDrone/VisDrone2019-DET-val/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.4GB ram): 100% 548/548 [00:05<00:00, 100.56it/s]\n",
            "Plotting labels to runs/train/exp5/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m2.95 anchors/target, 0.933 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING: Extremely small objects found. 29644 of 343201 labels are < 3 pixels in size.\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 342304 points...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7525: 100% 1000/1000 [01:01<00:00, 16.31it/s]\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9994 best possible recall, 5.81 anchors past thr\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.367/0.752-mean/best, past_thr=0.485-mean: 3,4, 4,9, 8,6, 7,14, 15,9, 15,19, 31,17, 25,37, 55,42\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp5\u001b[0m\n",
            "Starting training for 7 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/6      3.5G    0.1398    0.1445   0.05443       707       640: 100% 405/405 [02:33<00:00,  2.64it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 18/18 [00:26<00:00,  1.45s/it]\n",
            "                 all        548      38759     0.0122     0.0348    0.00767    0.00188\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/6      3.5G    0.1283    0.1606   0.04756       368       640: 100% 405/405 [02:26<00:00,  2.77it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 18/18 [00:20<00:00,  1.16s/it]\n",
            "                 all        548      38759      0.225     0.0745     0.0229    0.00635\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/6      3.5G    0.1213    0.1761   0.04453       713       640: 100% 405/405 [02:25<00:00,  2.79it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 18/18 [00:17<00:00,  1.06it/s]\n",
            "                 all        548      38759      0.363      0.104     0.0511     0.0158\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/6      3.5G    0.1165     0.178   0.04119       644       640: 100% 405/405 [02:24<00:00,  2.80it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 18/18 [00:15<00:00,  1.15it/s]\n",
            "                 all        548      38759      0.279      0.121     0.0678     0.0209\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/6      3.5G    0.1137    0.1777   0.03946       745       640: 100% 405/405 [02:24<00:00,  2.80it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 18/18 [00:16<00:00,  1.11it/s]\n",
            "                 all        548      38759      0.391      0.135     0.0821     0.0308\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/6      3.5G    0.1115    0.1768    0.0384       611       640: 100% 405/405 [02:24<00:00,  2.80it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 18/18 [00:15<00:00,  1.18it/s]\n",
            "                 all        548      38759      0.397      0.141     0.0941     0.0369\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/6      3.5G    0.1105    0.1744   0.03793       545       640: 100% 405/405 [02:24<00:00,  2.80it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 18/18 [00:15<00:00,  1.15it/s]\n",
            "                 all        548      38759      0.398      0.151     0.0984     0.0389\n",
            "\n",
            "7 epochs completed in 0.322 hours.\n",
            "Optimizer stripped from runs/train/exp5/weights/last.pt, 14.4MB\n",
            "Optimizer stripped from runs/train/exp5/weights/best.pt, 14.4MB\n",
            "\n",
            "Validating runs/train/exp5/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7037095 parameters, 0 gradients, 15.9 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 18/18 [00:32<00:00,  1.82s/it]\n",
            "                 all        548      38759      0.398      0.151     0.0984     0.0389\n",
            "          pedestrian        548       8844      0.112      0.311      0.119     0.0342\n",
            "              people        548       5125      0.218      0.204      0.123      0.033\n",
            "             bicycle        548       1287          1          0    0.00848    0.00197\n",
            "                 car        548      14064      0.283      0.636      0.517      0.243\n",
            "                 van        548       1975     0.0901      0.164     0.0528     0.0278\n",
            "               truck        548        750          0          0     0.0166    0.00764\n",
            "            tricycle        548       1045          1          0    0.00967    0.00304\n",
            "     awning-tricycle        548        532          1          0          0          0\n",
            "                 bus        548        251          0          0    0.00121   0.000909\n",
            "               motor        548       4886      0.278      0.193      0.137     0.0374\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 2571... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 ‚ñÅ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision ‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall ‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss ‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss ‚ñÅ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñá\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss ‚ñà‚ñá‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss ‚ñÅ‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 0.09837\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.03889\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.39818\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.15084\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.11055\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.03793\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.17436\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.10616\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.03935\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.24158\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 0.00269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 0.00269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.00269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 241 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mapricot-sea-8\u001b[0m: \u001b[34mhttps://wandb.ai/deepkotadia/YOLOv5/runs/1klnp9u2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211214_013959-1klnp9u2/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "Results saved to \u001b[1mruns/train/exp5\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train using VisDrone weights on custom coco\n",
        "%cd yolov5\n",
        "!python train.py --img 640 --batch 16 --epochs 7 --data coco_custom2_yolo.yaml --weights runs/train/exp5/weights/best.pt --cache\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxvp3KUHNe1E",
        "outputId": "d7fb14b2-acdb-48a6-df41-100f6a6c8acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeepkotadia\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=runs/train/exp5/weights/best.pt, cfg=, data=coco_custom2_yolo.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=7, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v6.0-141-ge8ef8fb torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mfancy-oath-9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/deepkotadia/YOLOv5/runs/1ew596xt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/yolov5/wandb/run-20211214_020619-1ew596xt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "Overriding model.yaml nc=10 with nc=20\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1     67425  models.yolo.Detect                      [20, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 270 layers, 7073569 parameters, 7073569 gradients, 16.0 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from runs/train/exp5/weights/best.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 57 weight, 60 weight (no decay), 60 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/train2017_custom2_yolo.cache' images and labels... 7771 found, 0 missing, 0 empty, 0 corrupted: 100% 7771/7771 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (6.8GB ram): 100% 7771/7771 [00:25<00:00, 303.81it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/val2017_custom2_yolo.cache' images and labels... 2961 found, 0 missing, 0 empty, 0 corrupted: 100% 2961/2961 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (2.6GB ram): 100% 2961/2961 [00:10<00:00, 285.70it/s]\n",
            "Plotting labels to runs/train/exp6/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m3.09 anchors/target, 0.758 Best Possible Recall (BPR). Anchors are a poor fit to dataset ‚ö†Ô∏è, attempting to improve...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mWARNING: Extremely small objects found. 178 of 46255 labels are < 3 pixels in size.\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mRunning kmeans for 9 anchors on 46254 points...\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.6909: 100% 1000/1000 [00:07<00:00, 127.42it/s]\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mthr=0.25: 0.9964 best possible recall, 4.31 anchors past thr\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mn=9, img_size=640, metric_all=0.290/0.692-mean/best, past_thr=0.472-mean: 11,12, 20,26, 25,61, 53,41, 53,105, 119,96, 120,235, 255,185, 391,355\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
            "Image sizes 640 train, 640 val\n",
            "Using 4 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp6\u001b[0m\n",
            "Starting training for 7 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/6      3.5G   0.09247   0.07165   0.06811       168       640: 100% 486/486 [03:01<00:00,  2.68it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:59<00:00,  1.56it/s]\n",
            "                 all       2961      16857      0.954    0.00774    0.00365   0.000787\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/6     6.17G   0.07768   0.07201   0.06056        81       640: 100% 486/486 [02:54<00:00,  2.79it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:34<00:00,  2.66it/s]\n",
            "                 all       2961      16857      0.276      0.044     0.0124     0.0034\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/6     6.17G   0.07252   0.07166   0.05721       110       640: 100% 486/486 [02:52<00:00,  2.81it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:41<00:00,  2.25it/s]\n",
            "                 all       2961      16857      0.296     0.0715     0.0253     0.0074\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       3/6     6.17G   0.06808   0.07141   0.05311        73       640: 100% 486/486 [02:52<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:34<00:00,  2.69it/s]\n",
            "                 all       2961      16857      0.162      0.171     0.0521     0.0163\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       4/6     6.17G   0.06626   0.07146   0.04894       133       640: 100% 486/486 [02:52<00:00,  2.83it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:29<00:00,  3.15it/s]\n",
            "                 all       2961      16857      0.224      0.165       0.08     0.0267\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       5/6     6.17G   0.06357   0.07081   0.04625        94       640: 100% 486/486 [02:52<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:34<00:00,  2.68it/s]\n",
            "                 all       2961      16857      0.277      0.172      0.113     0.0407\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       6/6     6.17G   0.06157   0.06946   0.04431       130       640: 100% 486/486 [02:52<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:28<00:00,  3.25it/s]\n",
            "                 all       2961      16857      0.214      0.202      0.119     0.0442\n",
            "\n",
            "7 epochs completed in 0.415 hours.\n",
            "Optimizer stripped from runs/train/exp6/weights/last.pt, 14.5MB\n",
            "Optimizer stripped from runs/train/exp6/weights/best.pt, 14.5MB\n",
            "\n",
            "Validating runs/train/exp6/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7064065 parameters, 0 gradients, 16.0 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:33<00:00,  2.75it/s]\n",
            "                 all       2961      16857      0.214      0.202      0.119     0.0442\n",
            "              person       2961       7141      0.239      0.481       0.35      0.127\n",
            "             bicycle       2961        314      0.103     0.0191     0.0106    0.00311\n",
            "                 car       2961       1843      0.174      0.314      0.198     0.0723\n",
            "          motorcycle       2961        367      0.171      0.188     0.0846      0.023\n",
            "            airplane       2961        143      0.282      0.385       0.31      0.103\n",
            "                 bus       2961        283      0.197      0.353      0.175     0.0735\n",
            "               train       2961        190      0.192      0.395       0.15     0.0572\n",
            "                boat       2961        424     0.0267    0.00943     0.0104    0.00249\n",
            "                bird       2961        427     0.0506     0.0117      0.014     0.0045\n",
            "                 cat       2961        202      0.242     0.0842     0.0927     0.0305\n",
            "                 dog       2961        218      0.242     0.0505     0.0861     0.0393\n",
            "               horse       2961        272      0.212      0.276      0.162      0.062\n",
            "               sheep       2961        354     0.0894      0.542      0.213     0.0845\n",
            "                 cow       2961        372      0.145      0.468      0.178     0.0817\n",
            "              bottle       2961       1013     0.0776     0.0168     0.0162    0.00492\n",
            "               chair       2961       1709      0.177     0.0328     0.0264    0.00875\n",
            "               couch       2961        261      0.145      0.069     0.0523     0.0174\n",
            "        potted plant       2961        342          1          0    0.00804    0.00241\n",
            "        dining table       2961        694      0.353     0.0591     0.0962     0.0316\n",
            "                  tv       2961        288      0.167      0.278      0.146      0.056\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 3328... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÜ‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall ‚ñÅ‚ñÇ‚ñÉ‚ñá‚ñá‚ñá‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss ‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss ‚ñá‚ñà‚ñá‚ñÜ‚ñá‚ñÖ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss ‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss ‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 ‚ñÇ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 0.11902\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 0.04421\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision 0.21447\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall 0.20158\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss 0.06157\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss 0.04431\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss 0.06946\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss 0.06278\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss 0.03935\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss 0.05321\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 0.00269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 0.00269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 0.00269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 241 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mfancy-oath-9\u001b[0m: \u001b[34mhttps://wandb.ai/deepkotadia/YOLOv5/runs/1ew596xt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211214_020619-1ew596xt/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "Results saved to \u001b[1mruns/train/exp6\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate on custom coco with transfer learning weights\n",
        "%cd yolov5\n",
        "!python val.py --weights runs/train/exp6/weights/best.pt --data coco_custom2_yolo.yaml --img 640 --iou 0.65 --half\n",
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSHHIuobOR0Z",
        "outputId": "fa91d5e7-5bfc-4f51-b391-8ae8a2ebf008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/data/coco_custom2_yolo.yaml, weights=['runs/train/exp6/weights/best.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 üöÄ v6.0-141-ge8ef8fb torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 213 layers, 7064065 parameters, 0 gradients, 16.0 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco_custom2_yolo/labels/val2017_custom2_yolo.cache' images and labels... 2961 found, 0 missing, 0 empty, 0 corrupted: 100% 2961/2961 [00:00<?, ?it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 93/93 [00:33<00:00,  2.75it/s]\n",
            "                 all       2961      16857      0.202      0.204      0.113     0.0423\n",
            "              person       2961       7141        0.2      0.492      0.332      0.121\n",
            "             bicycle       2961        314     0.0966     0.0191       0.01    0.00298\n",
            "                 car       2961       1843      0.149      0.323      0.186     0.0681\n",
            "          motorcycle       2961        367      0.151      0.191     0.0802     0.0216\n",
            "            airplane       2961        143      0.242      0.385      0.286     0.0956\n",
            "                 bus       2961        283      0.176      0.353      0.166     0.0704\n",
            "               train       2961        190      0.176        0.4      0.145     0.0549\n",
            "                boat       2961        424     0.0243    0.00943    0.00975    0.00232\n",
            "                bird       2961        427     0.0477     0.0117     0.0132    0.00418\n",
            "                 cat       2961        202      0.242     0.0842     0.0879     0.0294\n",
            "                 dog       2961        218      0.237     0.0505     0.0837     0.0381\n",
            "               horse       2961        272      0.199      0.276      0.158     0.0604\n",
            "               sheep       2961        354     0.0763      0.551      0.208     0.0821\n",
            "                 cow       2961        372      0.131       0.47      0.172     0.0795\n",
            "              bottle       2961       1013     0.0745     0.0168      0.015    0.00458\n",
            "               chair       2961       1709      0.171     0.0334     0.0249    0.00845\n",
            "               couch       2961        261      0.145      0.069     0.0506     0.0167\n",
            "        potted plant       2961        342          1          0    0.00725    0.00233\n",
            "        dining table       2961        694      0.339     0.0577     0.0927       0.03\n",
            "                  tv       2961        288      0.161      0.281      0.142     0.0544\n",
            "Speed: 0.1ms pre-process, 3.4ms inference, 3.1ms NMS per image at shape (32, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/val/exp2\u001b[0m\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8dHcni6CJYt"
      },
      "source": [
        "# Conclusion and Next Steps\n",
        "\n",
        "Congratulations! You've trained a custom YOLOv5 model to recognize your custom objects.\n",
        "\n",
        "To improve you model's performance, we recommend first interating on your datasets coverage and quality. See this guide for [model performance improvement](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results).\n",
        "\n",
        "To deploy your model to an application, see this guide on [exporting your model to deployment destinations](https://github.com/ultralytics/yolov5/issues/251).\n",
        "\n",
        "Once your model is in production, you will want to continually iterate and improve on your dataset and model via [active learning](https://blog.roboflow.com/what-is-active-learning/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7iiObB2WCMh6",
        "outputId": "8011f0de-2d2f-447f-fec9-91a18c33faf8"
      },
      "source": [
        "#export your model's weights for future use\n",
        "from google.colab import files\n",
        "\n",
        "%cd yolov5\n",
        "!zip -r custom_coco_val_results.zip runs/val/\n",
        "files.download('custom_coco_val_results.zip')\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "  adding: runs/val/ (stored 0%)\n",
            "  adding: runs/val/exp2/ (stored 0%)\n",
            "  adding: runs/val/exp2/val_batch2_pred.jpg (deflated 4%)\n",
            "  adding: runs/val/exp2/R_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp2/val_batch0_labels.jpg (deflated 12%)\n",
            "  adding: runs/val/exp2/F1_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp2/val_batch1_labels.jpg (deflated 5%)\n",
            "  adding: runs/val/exp2/val_batch2_labels.jpg (deflated 4%)\n",
            "  adding: runs/val/exp2/val_batch0_pred.jpg (deflated 12%)\n",
            "  adding: runs/val/exp2/confusion_matrix.png (deflated 12%)\n",
            "  adding: runs/val/exp2/val_batch1_pred.jpg (deflated 5%)\n",
            "  adding: runs/val/exp2/P_curve.png (deflated 4%)\n",
            "  adding: runs/val/exp2/PR_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp3/ (stored 0%)\n",
            "  adding: runs/val/exp3/val_batch2_pred.jpg (deflated 5%)\n",
            "  adding: runs/val/exp3/R_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp3/val_batch0_labels.jpg (deflated 12%)\n",
            "  adding: runs/val/exp3/F1_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp3/val_batch1_labels.jpg (deflated 6%)\n",
            "  adding: runs/val/exp3/val_batch2_labels.jpg (deflated 5%)\n",
            "  adding: runs/val/exp3/val_batch0_pred.jpg (deflated 12%)\n",
            "  adding: runs/val/exp3/confusion_matrix.png (deflated 12%)\n",
            "  adding: runs/val/exp3/val_batch1_pred.jpg (deflated 5%)\n",
            "  adding: runs/val/exp3/P_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp3/PR_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp/ (stored 0%)\n",
            "  adding: runs/val/exp/val_batch2_pred.jpg (deflated 4%)\n",
            "  adding: runs/val/exp/R_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp/val_batch0_labels.jpg (deflated 12%)\n",
            "  adding: runs/val/exp/F1_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp/val_batch1_labels.jpg (deflated 5%)\n",
            "  adding: runs/val/exp/val_batch2_labels.jpg (deflated 4%)\n",
            "  adding: runs/val/exp/val_batch0_pred.jpg (deflated 12%)\n",
            "  adding: runs/val/exp/confusion_matrix.png (deflated 12%)\n",
            "  adding: runs/val/exp/val_batch1_pred.jpg (deflated 5%)\n",
            "  adding: runs/val/exp/P_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp/PR_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp4/ (stored 0%)\n",
            "  adding: runs/val/exp4/val_batch2_pred.jpg (deflated 5%)\n",
            "  adding: runs/val/exp4/R_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp4/val_batch0_labels.jpg (deflated 12%)\n",
            "  adding: runs/val/exp4/F1_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp4/val_batch1_labels.jpg (deflated 6%)\n",
            "  adding: runs/val/exp4/val_batch2_labels.jpg (deflated 5%)\n",
            "  adding: runs/val/exp4/val_batch0_pred.jpg (deflated 12%)\n",
            "  adding: runs/val/exp4/confusion_matrix.png (deflated 12%)\n",
            "  adding: runs/val/exp4/val_batch1_pred.jpg (deflated 5%)\n",
            "  adding: runs/val/exp4/P_curve.png (deflated 4%)\n",
            "  adding: runs/val/exp4/PR_curve.png (deflated 4%)\n",
            "  adding: runs/val/exp5/ (stored 0%)\n",
            "  adding: runs/val/exp5/val_batch2_pred.jpg (deflated 4%)\n",
            "  adding: runs/val/exp5/R_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp5/val_batch0_labels.jpg (deflated 12%)\n",
            "  adding: runs/val/exp5/F1_curve.png (deflated 3%)\n",
            "  adding: runs/val/exp5/val_batch1_labels.jpg (deflated 5%)\n",
            "  adding: runs/val/exp5/val_batch2_labels.jpg (deflated 4%)\n",
            "  adding: runs/val/exp5/val_batch0_pred.jpg (deflated 12%)\n",
            "  adding: runs/val/exp5/confusion_matrix.png (deflated 12%)\n",
            "  adding: runs/val/exp5/val_batch1_pred.jpg (deflated 4%)\n",
            "  adding: runs/val/exp5/P_curve.png (deflated 4%)\n",
            "  adding: runs/val/exp5/PR_curve.png (deflated 3%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d7e385aa-049a-4f12-8027-45cec2178f2b\", \"custom_coco_val_results.zip\", 322017184)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vlo-fgeUEYIZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}